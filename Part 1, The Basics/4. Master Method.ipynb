{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master method\n",
    "\n",
    "In this notebook we will look at Master Method, a technique used to determine the running time of recursive algorithms. We plug in some characteristics of the recursive algorithms and we get an upper bound on the running time of the algorithm.\n",
    "\n",
    "We will revist Recursive Integer multiplication and Karatsuba multiplication introduced in first chapter. To recap, we split two number x and y of n digits into smaller numbers a, b, c and d each of length $\\frac{n}{2}$.\n",
    "\n",
    "$x\\:=\\: a\\cdot 10^{\\frac{n}{2}} + b\\:\\:$ and $\\:\\:y\\:=\\: c\\cdot 10^{\\frac{n}{2}} + d\\:\\:$\n",
    "Thus\n",
    "\n",
    "$x.y = (a\\cdot 10^{\\frac{n}{2}} + b) \\cdot (c\\cdot 10^{\\frac{n}{2}} + d) = 10^n\\cdot (a\\cdot c) + 10^{\\frac{n}{2}}\\cdot (a\\cdot d + b\\cdot c) + b\\cdot d$\n",
    "\n",
    "Expressing $T(n)$ in terms of recursive calls we get\n",
    "\n",
    "$T(n) \\leq 4\\cdot T(\\frac{n}{2}) + O(n)$\n",
    "\n",
    "Since the input is halved on each recursive call, we recursively operator on 4 smaller problems. After the 4 recursive calls we do linear amount of work outside the recursive calls.\n",
    "\n",
    "For the karatsuba multiplication, we apply gausses trick to reduce one recursive call by representing two multiplication as a linear combination of others. Thus expressing $T(n)$ as above we get\n",
    "\n",
    "$T(n) \\leq 3\\cdot T(\\frac{n}{2}) + O(n)$\n",
    "\n",
    "Thus intutively, karatsuba miltiplication seems better than the standard divide and conquer recursive technique though the linear work to be performed has slightly more constant multiple than the recursive multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "For master method, we express the recursive problems problem as follows\n",
    "\n",
    "$T(n)\\: \\leq \\:  a\\cdot T(\\frac{n}{b}) + O(n^d)$\n",
    "\n",
    "After several recursive calls, the base case will return the answer in constant time. The constants a, b and d are defined as\n",
    "\n",
    "a = Number of recursive calls on smaller input\n",
    "b = The input shrinkage factor\n",
    "d = exponent of running time for the work done in the combine phase after the recursive calls.\n",
    "\n",
    "The master method assumes that the input shrinks at a fixed rate and the smaller subproblems operate on a similar sized problems. This assumption holds true for a large category of divide and conquer problems. For example, in case of merge sort the input is halved and we have two recursive calls on smaller problems. Outside these recursive calls, we perform linear work. Thus for merge sort **a, b ** are **2** and **d** is **1**\n",
    "\n",
    "For master method we have\n",
    "\n",
    "$$T(n) = \n",
    "\\begin{cases}\n",
    "    O(n^d log\\: n)  & \\text{if } a = b^d \\\\\n",
    "    O(n^d)          & \\text{if } a < b^d \\\\\n",
    "    O(n^{log_b\\: a})& \\text{if } a > b^d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The value $a, b > 0$ and $d\\geq 0$. The value of d being 0 shows constant work done outside the recursive calls.\n",
    "\n",
    "\n",
    "***Also these constants are not dependent on the value n***.\n",
    "\n",
    "With these cases defined, let us apply this to some of the algorithms we know.\n",
    "\n",
    "- Merge Sort: We know the Merge Sort runs in $O(nlogn)$ time. The value of a b and d for merge sort are 2, 2 and 1 respectively. Since $a = b^d$, we are in case 1 of master method and thus we get $O(n) = nlogn$\n",
    "\n",
    "- Binary Search: Binary search works on half the input size on each recursive call and only involves one recursive call as we either operate on the left or right half of the numbers depending on the number to be searched. Also the work done outside recursive is done in constant time. Thus we get value of a, b and d as 1, 2 and 0 respectively. This again is case 1 and we get the running time of $logn$\n",
    "\n",
    "- Recursive Multiplication: As we saw earlier the value of a, b and d 4, 2 and 1 respectively. This is case 3 of master method and thus we get $O(n) = n ^{log_2 4}$. Since $log_2 4 = 2$ we have $O(n) = n^2$\n",
    "\n",
    "- Karatsuba multiplication: In this case we saw that the value of a, b and d are 3, 2, 1 respectively. This again is case 3 and thus $O(n) = n ^{log_2 3}$. The value of $log_2 3 \\approx 1.59$ and thus we have the upper bound as $O(n) = n^{1.59}$. This te karatsuba multiplication performs multiplication in sub quadratic time.\n",
    "\n",
    "\n",
    "- For Matrix multiplication, we saw that any $n \\times n$ matrix is broken into two $\\frac{n}{2}\\times \\frac{n}{2}$ matrix. The work done outside is quardartic. Also we had 8 matrix multiplications of smaller inputs each of size $\\frac{n}{2}$. Thus, we have a, b and c as 8, 2 and 2 respectively. This again is case 3 and we have $O(n) = n ^{log_2 8}$. Since $log_2 8 = 3$ we have $O(n) = n^3$\n",
    "\n",
    "- Strassen's Matrix multiplication: This approach similar to gausses trick in Karatsuba multiplication, manages to reduce the number of recursive calls from 8 to 7. Thus we have a, b and c as 7, 2 and 2 respectively. Thus we have\n",
    " $O(n) = n ^{log_2 7}$. Since $log_2 7 \\approx 2.81$ we have $O(n) = n^{2.81}$\n",
    " \n",
    " - Fictitious case: TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
